{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'weteh-sagemaker-examples'\n",
    "prefix = 'sagemaker/DEMO-linear-mnist'\n",
    " \n",
    "# Define IAM role\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = \"arn:aws:iam::869530972998:role/SagemakerAdmin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, gzip, numpy, urllib.request, json\n",
    "\n",
    "# load MNIST dataset from deepleaerning.net website as pickle format\n",
    "# https://docs.python.org/3/library/pickle.html\n",
    "# pickle is a python serder module that serializes/deserializes python data structures between binary form and\n",
    "# python formats\n",
    "\n",
    "\n",
    "urllib.request.urlretrieve(\"http://deeplearning.net/data/mnist/mnist.pkl.gz\", \"mnist.pkl.gz\")\n",
    "\n",
    "# you can use gzip python library to deserialize pickle files\n",
    "# the output are in the following format and structure:\n",
    "# train_set: 50000 examples\n",
    "# test_set: 10000 examples\n",
    "# validation set: 10000 examples\n",
    "# data structure: tuple (2 dimensions with features, labels)\n",
    "# features has ndarray with 784 dimensions (28 x 28), single channel.\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_digit(img, caption='', subplot=None):\n",
    "    if not subplot:\n",
    "        _,(subplot)=plt.subplots(1,1) # subplots returns Figure and Axes, we don't care about Figure.\n",
    "    img = img.reshape((28,28))\n",
    "    subplot.imshow(img, cmap='gray')\n",
    "    plt.title(caption)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "img_idx = random.randrange(0, len(train_set[0]))\n",
    "show_digit(train_set[0][img_idx], 'this is a {}'.format(train_set[1][img_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import sagemaker.amazon.common as smac\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "key = 'recordio-pb-data'\n",
    "\n",
    "# We need to convert the original pickled array into recordIO so that sagemaker can process the dataset at training time\n",
    "# not sure if we need to use .tolist() in the list comprehension.\n",
    "# Basically it's converting a flatten structure with float32 datatypes into recordIO dformat\n",
    "def write_recordio(data, data_type = 'train'):\n",
    "    vectors = np.array([t.tolist() for t in data[0]]).astype('float32')\n",
    "\n",
    "    # In this example, we are only training for binary classification, where if it's 0 then it's a 1, otherwise it's a 0\n",
    "    labels = np.where(np.array([t.tolist() for t in data[1]]) == 0, 1, 0).astype('float32')\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_numpy_to_dense_tensor(buf, vectors, labels)\n",
    "    buf.seek(0)\n",
    "    # for 50k records, the data size is around 151MB    \n",
    "    boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, data_type, key)).upload_fileobj(buf)\n",
    "    s3_data = 's3://{}/{}/{}/{}'.format(bucket, prefix, data_type, key)\n",
    "    print('uploaded {} data location: {}'.format(data_type, s3_data))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_recordio(train_set, data_type='train')\n",
    "write_recordio(valid_set, data_type='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('training artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "# '382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner:1'\n",
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup sagemaker session\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "#https://sagemaker.readthedocs.io/en/stable/estimators.html\n",
    "model = sagemaker.estimator.Estimator(container, \n",
    "                                      train_instance_type='ml.c4.xlarge',\n",
    "                                      role=role, \n",
    "                                      train_instance_count=1,\n",
    "                                      sagemaker_session=sess,\n",
    "                                      output_path=output_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html\n",
    "model.set_hyperparameters(feature_dim = len(train_set[0][0]),\n",
    "                          predictor_type='binary_classifier',\n",
    "                          mini_batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = {\n",
    "    'train': 's3://{}/{}/train/{}'.format(bucket, prefix, key),\n",
    "    'validation' : 's3://{}/{}/validation/{}'.format(bucket, prefix, key)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test input as CSV for batch transform\n",
    "import numpy \n",
    "test_vectors = np.array([t.tolist() for t in test_set[0]]).astype('float32')\n",
    "test_csv_filename = 'test.csv'\n",
    "numpy.savetxt(test_csv_filename, test_vectors, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_s3 = sess.upload_data(test_csv_filename,\n",
    "                           bucket=bucket,\n",
    "                           key_prefix='{}/test'.format(prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a transformer from the trained model\n",
    "model_transformer = model.transformer(instance_count=1,\n",
    "                                  instance_type='ml.m4.xlarge',\n",
    "                                  strategy='MultiRecord',\n",
    "                                  assemble_with='Line',\n",
    "                                  output_path='s3://{}/{}/predictions'.format(bucket, prefix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transformer.transform(test_s3, content_type='text/csv', split_type='Line')\n",
    "model_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=bucket, Key='{}/predictions/{}.out'.format(prefix, test_csv_filename))\n",
    "df = pd.read_json(io.BytesIO(obj['Body'].read()), lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "predictions = df['predicted_label'].to_numpy()\n",
    "true_test_labels = np.where(test_set[1] == 0, 1, 0)\n",
    "pd.crosstab(true_test_labels, predictions, rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_ml import ConfusionMatrix\n",
    "confusion_matrix = ConfusionMatrix(true_test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix:\\n%s\" % confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_matrix.plot(normalized=True)\n",
    "confusion_matrix.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
